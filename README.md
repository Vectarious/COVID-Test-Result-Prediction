# COVID-Test-Result-Prediction
Machine learning algorithm to determine the result of a COVID-19 test result based on 10+ variables (Fall 2021)

** Note: The program takes 4-5 minutes to run through the tree and test the data! Results will be printed to the terminal at the end of the process.

## Introduction
For this project, I used machine learning algorithms to predict whether or not someone
will test positive for covid based on several variables. While covid testing has gradually become extremely widespread and accessible over the past 2 years, there was a time early in the pandemic where testing was scarce, so data like this would be very beneficial in predicting whether or not someone has the virus without necessarily testing them. The data I used came from the Israeli Ministry of Health which publicly reports data about individuals tested for coronavirus. I found this dataset online, and it was actually used in another organization’s machine learning project (I have linked their report in my code). To this point I haven’t taken a look at their methodology or results since I didn’t want their work to impact the way I made my program—I just used their GitHub to download the dataset so I could implement my own algorithm with it. The datafile contains over 270,000 test results, and includes the following information for each datapoint: `test_date`, `cough`, `fever`, `sore_throat`, `shortness_of_breath`, `head_ache`, `corona_result`, `age_60_and_above`, `gender`, and `test_indication`. Most of these categories are booleans, but some are not; this proved to be somewhat complicated to resolve, but in the end I believe I found a nice way to sort through it all and clean the data.

## Methodology
For my methodology, I felt that this data called for a tree algorithm, since there are only a dozen or so variables to work with and it was clear that some would clearly be more indicative than others (for example, `gender` is probably not very indicative of whether or not someone will test
 positive for covid). I therefore started by creating a `BTNode` class which I could use to procedurally generate a tree structure. I then needed to clean my dataset—I decided that I wanted to turn any columns with more than 2 potential options into multiple booleans. My `clean_data` function does a very good job with doing this efficiently. Essentially as I read in the data from the csv in my main function, it sends each row of data to the `clean_data` function and assigns each index of the row to a variable. It then returns a new list which contains only booleans corresponding to the variables. For example, one variable which has multiple options is the `test_indication` variable (“abroad”, “contact with confirmed”, and “other”). I simply turned this into 3 columns where each contains trues only where a given single option is true. While the logic for this took me some time to figure out, I think it’s a very efficient and clean way to go about storing the dataset.


I then needed to procedurally create my tree for the dataset. This is obviously done recursively, and the base case is reached when the tree is out of variables to split by, or if the dataset has been emptied. Once this has been reached, the proportion of positives to total remaining test points is stored in the leaf. In the recursive case, the best variable to split the set is found using the `find_best_variable` function, and then the remaining dataset is split based on that variable using the `make_smaller_dataset` function. Both of these functions are described in the code. In these non-leaf nodes, the variable is stored which allows for the `recurse_tree` function to easily maneuver through the split-points and arrive at a leaf. The `find_best_variable_helper` function determines which variable is best by determining which side has the higher ratio of positives to total points, and then comparing this value with the higher value for each variable split. After a lot of thought, I determined that this was the most indicative of a good variable split (since there are so many more negative results than positive ones, getting a high percentage of positives on one side is critical for making a conclusion).

## Validation
Now that the trees could be made and datapoints could be passed through them to get probabilities of being positive, it was time to validate the structure, specifically through leave-one-out validation. Unfortunately, given that there were hundreds of thousands of datapoints, and since running a single point took about 3-4 seconds, it would take roughly a week for my laptop to make it through the whole set. I therefore did several test runs with 50 or 100 points apiece and got between 92-98% accuracy. While I could have done this a few more times and called it a day, I wanted to get a better sense for how it did with all points. While technically not the correct way to validate using this method, I ran each point through the same tree (generated with all points), rather than leaving the test point out before creating the tree. In most instances, this would yield extremely biased results, since leaving the test point itself in the tree would mean it would be validating itself. However, since there are only 11 variables (meaning there are only 2^11 = 2048 possibilities), and since there are over 270k datapoints, there are many duplicate points which dramatically lowers the ability for a single point to influence the outcome (outlier points are virtually impossible). Therefore, while it isn’t perfect, I felt it should give a very good idea for how well the algorithm is at testing for covid within a reasonable timeframe.

## Troubling Initial Results...
After running through all the points with the `validate_dataset` function, I found that my algorithm was 97% accurate at predicting the result. This seemed insanely high to me, so I thought about why this might be for a while longer and came to realize it was likely due to the overwhelming number of negative results. Up to this point, I had been using `comparison_value = 0.5`, since I thought this made the most logical sense (if there were more than 50% positive values, it should guess a positive result, and vice versa with negative results). However, since there are so many more negative results, getting 50% or higher for positive results is very rare. I made the `validate_positives` function to find the percentage of positive results I was successfully predicting, and found it to be roughly 58%. While I was predicting 99% of negative results correctly, this low rate for positives rendered the entire algorithm useless.

## Actual Results
I toyed with the `comparison_value` (and made it an input variable rather than a hardcoded 0.5), and found that 0.02 was an ideal value which correctly predicted both positive and negative results with ***~85% accuracy***. Essentially this change made it so that instead of giving a lot of false negatives and very few false positives, it gave an equal percentage of false outcomes independent of result. While this decreases the overall accuracy of the model, it increases its usefulness tremendously. I then wrote a pseudo-Naive Bayes algorithm to predict the same results. While the way I created this process is technically "wrong," this was done knowingly and I believe it can be justified. Much like the second way I did the tree validator, instead of removing the datapoint I was testing, I left it included in order to save time. Removing each would have taken far too long to run for every point, but given that there are so many datapoints, the probabilities aren’t changed noticeably if the original point is left in the calculation. Therefore, this method seemed to get the job done successfully while sacrificing minimal accuracy. I initially expected poorer results from this algorithm given that the model is much simpler than the tree and doesn’t weigh important features as heavily, but this turned out to be a false assumption. While the tree was still the more accurate model, the Naive Bayes classifier wasn’t far off with ***~84% accuracy***.
